{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dce4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\".datasets\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f71d4d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAANnElEQVR4nO3dX6xV9ZnG8edRW/9RIwzgMBanBbkYNcaOBCcpESe16HghVNMREieIzdCYatqkJhrGWBM1aSbTNt7YBNBAR0aDAQc0zVhCqsgN8WgYRbFFCdPSQ8CGGCzRMMI7F2cxOcWzf+uw/60N7/eTnOx91rvXXm/24WGtvX97rZ8jQgDOfGc13QCA/iDsQBKEHUiCsANJEHYgiXP6uTHbfPQP9FhEeKzlHe3Zbd9s+ze237f9YCfPBaC33O44u+2zJf1W0jcl7ZP0uqTFEfFuYR327ECP9WLPPkfS+xGxJyKOSnpO0oIOng9AD3US9ksl/X7U7/uqZX/G9jLbQ7aHOtgWgA518gHdWIcKnztMj4gVklZIHMYDTepkz75P0vRRv39Z0nBn7QDolU7C/rqkWba/avuLkhZJ2tSdtgB0W9uH8RHxme17Jb0s6WxJT0fEO13rDEBXtT301tbGeM8O9FxPvlQD4PRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii7fnZJcn2XkkfSzom6bOImN2NpgB0X0dhr/x9RPyxC88DoIc4jAeS6DTsIelXtt+wvWysB9heZnvI9lCH2wLQAUdE+yvbfxURw7anStos6b6I2Fp4fPsbAzAuEeGxlne0Z4+I4er2oKQXJM3p5PkA9E7bYbd9oe0vnbgvab6knd1qDEB3dfJp/CWSXrB94nn+IyL+qytdAei6jt6zn/LGeM8O9FxP3rMDOH0QdiAJwg4kQdiBJAg7kEQ3ToTBALvuuuuK9TvvvLNYnzdvXrF+5ZVXnnJPJ9x///3F+vDwcLE+d+7cYv2ZZ55pWdu+fXtx3TMRe3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKz3s4Ad9xxR8vaE088UVx38uTJxXp1CnNLr7zySrE+ZcqUlrUrrriiuG6dut6ef/75lrVFixZ1tO1BxllvQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE57MPgHPOKf8ZZs8uT467cuXKlrULLriguO7WrS0n8JEkPfroo8X6tm3bivVzzz23ZW3dunXFdefPn1+s1xkaYsax0dizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMPgLprt69atart5968eXOxXjoXXpIOHz7c9rbrnr/TcfR9+/YV62vWrOno+c80tXt220/bPmh756hlk2xvtr27up3Y2zYBdGo8h/GrJd180rIHJW2JiFmStlS/AxhgtWGPiK2SDp20eIGkE8dIayQt7G5bALqt3ffsl0TEfkmKiP22p7Z6oO1lkpa1uR0AXdLzD+giYoWkFRIXnASa1O7Q2wHb0ySpuj3YvZYA9EK7Yd8kaUl1f4mkjd1pB0Cv1F433vazkm6QNFnSAUk/kvSfktZJukzS7yR9OyJO/hBvrOdKeRhfd0748uXLi/W6v9GTTz7ZsvbQQw8V1+10HL3Orl27WtZmzZrV0XPffvvtxfrGjTn3Qa2uG1/7nj0iFrcofaOjjgD0FV+XBZIg7EAShB1IgrADSRB2IAlOce2Chx9+uFivG1o7evRosf7yyy8X6w888EDL2ieffFJct855551XrNedpnrZZZe1rNVNufzYY48V61mH1trFnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkqg9xbWrGzuNT3G9+OKLW9bee++94rqTJ08u1l966aVifeHChcV6Jy6//PJife3atcX6tdde2/a2169fX6zffffdxfqRI0fa3vaZrNUpruzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnHaerUljNcaXh4uKPnnjFjRrH+6aefFutLly5tWbv11luL61511VXF+oQJE4r1un8/pfptt91WXPfFF18s1jE2xtmB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAnG2cepdD57aVpiSZoyZUqxXnf99F7+jeq+I1DX27Rp04r1Dz/8sO110Z62x9ltP237oO2do5Y9YvsPtndUP7d0s1kA3Teew/jVkm4eY/nPIuKa6ueX3W0LQLfVhj0itko61IdeAPRQJx/Q3Wv7reowf2KrB9leZnvI9lAH2wLQoXbD/nNJMyVdI2m/pJ+0emBErIiI2RExu81tAeiCtsIeEQci4lhEHJe0UtKc7rYFoNvaCrvt0WMm35K0s9VjAQyG2vnZbT8r6QZJk23vk/QjSTfYvkZSSNor6bu9a3EwfPTRRy1rddd1r7su/KRJk4r1Dz74oFgvzVO+evXq4rqHDpU/e33uueeK9bqx8rr10T+1YY+IxWMsfqoHvQDoIb4uCyRB2IEkCDuQBGEHkiDsQBK1n8aj3vbt24v1ulNcm3T99dcX6/PmzSvWjx8/Xqzv2bPnlHtCb7BnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdP7vzzzy/W68bR6y5zzSmug4M9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwZTNKDp27FixXvfvp3Sp6dJ0zmhf21M2AzgzEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpzPntxNN93UdAvok9o9u+3ptn9te5ftd2x/v1o+yfZm27ur24m9bxdAu8ZzGP+ZpB9GxN9I+jtJ37N9haQHJW2JiFmStlS/AxhQtWGPiP0R8WZ1/2NJuyRdKmmBpDXVw9ZIWtijHgF0wSm9Z7f9FUlfk7Rd0iURsV8a+Q/B9tQW6yyTtKzDPgF0aNxhtz1B0npJP4iIw/aY37X/nIhYIWlF9RycCAM0ZFxDb7a/oJGgr42IDdXiA7anVfVpkg72pkUA3VC7Z/fILvwpSbsi4qejSpskLZH04+p2Y086RE/NmDGj6RbQJ+M5jP+6pH+S9LbtHdWy5RoJ+Trb35H0O0nf7kmHALqiNuwRsU1Sqzfo3+huOwB6ha/LAkkQdiAJwg4kQdiBJAg7kASnuCb32muvFetnnVXeH9RN6YzBwZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnD25nTt3Fuu7d+8u1uvOh585c2bLGlM29xd7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhH9m6SFGWFOP3fddVexvmrVqmL91VdfbVm77777iuu+++67xTrGFhFjXg2aPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJFE7zm57uqRfSPpLScclrYiIJ2w/IumfJZ04KXl5RPyy5rkYZz/NXHTRRcX6unXrivUbb7yxZW3Dhg3FdZcuXVqsHzlypFjPqtU4+3guXvGZpB9GxJu2vyTpDdubq9rPIuLfutUkgN4Zz/zs+yXtr+5/bHuXpEt73RiA7jql9+y2vyLpa5K2V4vutf2W7adtT2yxzjLbQ7aHOmsVQCfGHXbbEyStl/SDiDgs6eeSZkq6RiN7/p+MtV5ErIiI2RExu/N2AbRrXGG3/QWNBH1tRGyQpIg4EBHHIuK4pJWS5vSuTQCdqg27bUt6StKuiPjpqOXTRj3sW5LKlykF0KjxDL3NlfSapLc1MvQmScslLdbIIXxI2ivpu9WHeaXnYujtDFM3NPf444+3rN1zzz3Fda+++upinVNgx9b20FtEbJM01srFMXUAg4Vv0AFJEHYgCcIOJEHYgSQIO5AEYQeS4FLSwBmGS0kDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBLjubpsN/1R0v+M+n1ytWwQDWpvg9qXRG/t6mZvf92q0Ncv1Xxu4/bQoF6bblB7G9S+JHprV7964zAeSIKwA0k0HfYVDW+/ZFB7G9S+JHprV196a/Q9O4D+aXrPDqBPCDuQRCNht32z7d/Yft/2g0300Irtvbbftr2j6fnpqjn0DtreOWrZJNubbe+ubsecY6+h3h6x/Yfqtdth+5aGeptu+9e2d9l+x/b3q+WNvnaFvvryuvX9PbvtsyX9VtI3Je2T9LqkxRExEFf8t71X0uyIaPwLGLavl/QnSb+IiKuqZf8q6VBE/Lj6j3JiRDwwIL09IulPTU/jXc1WNG30NOOSFkq6Sw2+doW+/lF9eN2a2LPPkfR+ROyJiKOSnpO0oIE+Bl5EbJV06KTFCyStqe6v0cg/lr5r0dtAiIj9EfFmdf9jSSemGW/0tSv01RdNhP1SSb8f9fs+DdZ87yHpV7bfsL2s6WbGcMmJabaq26kN93Oy2mm8++mkacYH5rVrZ/rzTjUR9rGujzVI439fj4i/lfQPkr5XHa5ifMY1jXe/jDHN+EBod/rzTjUR9n2Spo/6/cuShhvoY0wRMVzdHpT0ggZvKuoDJ2bQrW4PNtzP/xukabzHmmZcA/DaNTn9eRNhf13SLNtftf1FSYskbWqgj8+xfWH1wYlsXyhpvgZvKupNkpZU95dI2thgL39mUKbxbjXNuBp+7Rqf/jwi+v4j6RaNfCL/gaR/aaKHFn3NkPTf1c87Tfcm6VmNHNb9r0aOiL4j6S8kbZG0u7qdNEC9/btGpvZ+SyPBmtZQb3M18tbwLUk7qp9bmn7tCn315XXj67JAEnyDDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D+B61FSWV/i6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with gzip.open((PATH/FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "plt.imshow(x_train[4].reshape((28,28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcaa6b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 8, 4, 8], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "x_valid_tensor = torch.from_numpy(x_valid)\n",
    "y_valid_tensor = torch.from_numpy(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7edc021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1416, grad_fn=<NegBackward>) tensor(0.7344)\n",
      "tensor(0.6729, grad_fn=<NegBackward>) tensor(0.8594)\n"
     ]
    }
   ],
   "source": [
    "# NN from scratch\n",
    "# we initialize weigths with Xavier initialisation by multiplying by 1 / sqrt(n)\n",
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "# 784 (x) neurons and each neuron generate 10 output (y)\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias) # @ is dot product\n",
    "\n",
    "# loss function\n",
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = nll\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i] # get mini batch\n",
    "        yb = y_train_tensor[start_i:end_i] # get mini batch\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d788287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0767, grad_fn=<NllLossBackward>) tensor(0.7656)\n",
      "tensor(0.6670, grad_fn=<NllLossBackward>) tensor(0.8281)\n"
     ]
    }
   ],
   "source": [
    "# NN with torch.nn.functional\n",
    "# we initialize weigths with Xavier initialisation by multiplying by 1 / sqrt(n)\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# 784 (x) neurons and each neuron generate 10 output (y)\n",
    "def model(xb):\n",
    "    return xb @ weights + bias # @ is dot product\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i] # get mini batch\n",
    "        yb = y_train_tensor[start_i:end_i] # get mini batch\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2d13854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1003, grad_fn=<NllLossBackward>) tensor(0.6875)\n",
      "tensor(0.6607, grad_fn=<NllLossBackward>) tensor(0.8906)\n"
     ]
    }
   ],
   "source": [
    "# NN with torch.nn.functional and torch.nn.Module\n",
    "# we initialize weigths with Xavier initialisation by multiplying by 1 / sqrt(n)\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i] # get mini batch\n",
    "        yb = y_train_tensor[start_i:end_i] # get mini batch\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        # Notice we dont expose model params anymore!\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters(): \n",
    "                p -= p.grad * lr\n",
    "            model.zero_grad()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c6643dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1207, grad_fn=<NllLossBackward>) tensor(0.6406)\n",
      "tensor(0.6351, grad_fn=<NllLossBackward>) tensor(0.9062)\n"
     ]
    }
   ],
   "source": [
    "# NN with torch.nn.functional and torch.nn.Module and nn.Linear\n",
    "# we initialize weigths with Xavier initialisation by multiplying by 1 / sqrt(n)\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10) # Notice we don't expose weights and bias params\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.linear(xb)\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i] # get mini batch\n",
    "        yb = y_train_tensor[start_i:end_i] # get mini batch\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters(): \n",
    "                p -= p.grad * lr\n",
    "            model.zero_grad()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "97deff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1159, grad_fn=<NllLossBackward>) tensor(0.6875)\n",
      "tensor(0.6683, grad_fn=<NllLossBackward>) tensor(0.8594)\n"
     ]
    }
   ],
   "source": [
    "# NN with torch.nn.functional and torch.nn.Module and nn.Linear and optim\n",
    "# we initialize weigths with Xavier initialisation by multiplying by 1 / sqrt(n)\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.linear(xb)\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # optimizer defined here\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i] # get mini batch\n",
    "        yb = y_train_tensor[start_i:end_i] # get mini batch\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        # Notice we don't have to loop over model parameters by hand\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca59a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1283, grad_fn=<NllLossBackward>) tensor(0.7031)\n",
      "tensor(0.6430, grad_fn=<NllLossBackward>) tensor(0.9219)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch has abstract class for custom dataset named Dataset. TensorDataset is wrapper for this class.\n",
    "# We can iterate both x and y set same time.\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.linear(xb)\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # optimizer defined here\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((100 - 1) // bs + 1): # every minibacth\n",
    "        xb,yb = train_ds[i*bs : i*bs+bs] # Notice how we iterate\n",
    "        pred = model(xb) # predictions\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        # Notice we don't have to loop over model parameters by hand\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bd5e197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1097, grad_fn=<NllLossBackward>) tensor(1.)\n",
      "tensor(0.0811, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch's DataLoader class is responsible for loading mini-batches. With help of this class we can\n",
    "# write much cleaner train loops.\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.linear(xb)\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # optimizer defined here\n",
    "train_ds = TensorDataset(x_train_tensor, y_train_tensor) #  Tensor dataset object\n",
    "train_dl = DataLoader(train_ds, batch_size=bs) # DataLoader object\n",
    "\n",
    "# OMG so clean!\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8340e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3090)\n",
      "1 tensor(0.2898)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.linear(xb)\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "\n",
    "# for evaluation\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "bs = 64  # batch size\n",
    "xb = x_train_tensor[0:bs]  # a mini-batch from x\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # optimizer defined here\n",
    "\n",
    "#  Tensor dataset object we also shuffle the dataset here\n",
    "train_ds = TensorDataset(x_train_tensor, y_train_tensor) \n",
    "train_dl = DataLoader(train_ds, batch_size=bs) # DataLoader object\n",
    "\n",
    "valid_ds = TensorDataset(x_valid_tensor, y_valid_tensor) # No need shuffle for validation set\n",
    "# We can use larger batch size because validation don't need calc back prop so uses less memory.\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "# OMG so clean!\n",
    "for epoch in range(epochs):\n",
    "    model.train() # this phases are important while using batchnorm and dropout\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval() # this phases are important while using batchnorm and dropout\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "    print(epoch, valid_loss / len(valid_dl))\n",
    "    #print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "25136185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5711, grad_fn=<NllLossBackward>) tensor(0.8906)\n"
     ]
    }
   ],
   "source": [
    "xb_valid = x_valid_tensor[:64]\n",
    "yb_valid = y_valid_tensor[:64]\n",
    "\n",
    "print(loss_func(model(xb_valid), yb_valid), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52fb37f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fdc3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2720b9e6fa0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb_valid[0].reshape(28, 28)\n",
    "plt.imshow(xb_valid[0].reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec40af73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6155, -0.9929, -1.5386, -2.7747, -3.2980, -3.3874, -3.2308, -3.4792,\n",
       "         -2.1557, -3.8000]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model(torch.unsqueeze(xb_valid[0], dim=0))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9805714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torch.nn.Module\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e77c98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()\n",
    "opt = optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6c32570",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((100-1)//bs+1): # every minibatch\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train_tensor[start_i:end_i]\n",
    "        yb = y_train_tensor[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92b3a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9465, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= p.grad * lr\n",
    "        model.zero_grad()\n",
    "this block can supress with follwing lines:\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bab070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775f8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8500d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bfd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
